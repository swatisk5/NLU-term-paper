# NLU-term-paper
BPE vs unigramLM
Unigram model
Explaination:
The code divides sentences or words into smaller chunks known as tokens. The `UnigramTokenizer` class is set up with two main parameters: the name of a language model that has already been trained (for example, "xlm-roberta-base") and the path to a file that has a corpus of text in the target language (for example, "odia_sentences.txt"). Inside this class, various dictionaries are created to store information about the frequency of words, characters, and subwords in the corpus, as well as the model itself.
Calculating frequencies: The code reads each line of the corpus file and uses the given language model to tokenize it. Based on that, it figures out how often each word, character, and subword appears in the corpus. This information is very important for later building the model.
Model building: The code builds a unigram model after figuring out the frequencies. Each subword in this model is given a score based on how often it appears in the corpus. A logarithmic function is used to figure out the score based on how often the subword appears compared to all the other subwords.
Tokenization:
   - The `encode_word` method takes a word as input and breaks it down into subwords based on the model constructed earlier.
   - The `tokenize` method takes a piece of text as input, tokenizes it into words, and then further breaks down each word into subwords using the `encode_word` method.
Evaluation: - The code can determine the model's loss and scores. In terms of how well it represents the language, these metrics help figure out how well the model is doing. Text from an input file is read by the `tokenize_file` method, which uses the model to turn it into tokens. The tokenized text is then written to an output file.
BPE model
The provided code introduces a class named BPETokenizer, which encapsulates the functionality to train a tokenizer based on Byte Pair Encoding (BPE) from a given text corpus and then utilize this trained tokenizer to tokenize text. The class is equipped with methods to accomplish tasks such as reading a corpus from a file, tokenizing text using the BPE tokenizer, computing pair frequencies, merging pairs of characters based on frequency, training the BPE tokenizer, applying the trained tokenizer to tokenize text, and saving the generated vocabulary to a file.
BPETokenizer Class: This class is the core of the code. It's designed to tokenize text using Byte Pair Encoding (BPE).
Initialization: In the `__init__` method, the class initializes parameters like `vocab_size`, `word_freqs`, `merges`, and creates an instance of `ByteLevelBPETokenizer`. 
Tokenization: The `tokenize_text` method takes a text string as input and returns a list of tokens generated by the Byte Level BPE tokenizer.
Computing Pair Frequencies: This method, `compute_pair_freqs`, calculates the frequency of pairs of characters in the corpus.
Merging Pairs:The `merge_pair` method merges the most frequent pairs of characters in the corpus.
Training BPE: The `train_bpe` method trains the BPE tokenizer using the corpus.

Applying BPE: The `apply_bpe` method applies the trained BPE tokenizer to tokenize a given text.
Saving Vocabulary: The `save_vocab` method saves the vocabulary generated by the BPE tokenizer to a file.
