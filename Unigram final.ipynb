{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83206ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "import pandas as pd\n",
    "from math import log\n",
    "\n",
    "class UnigramTokenizer:\n",
    "    def __init__(self, model_name, corpus_file_path):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.corpus_file_path = corpus_file_path\n",
    "        self.word_freqs = defaultdict(int)\n",
    "        self.char_freqs = defaultdict(int)\n",
    "        self.subwords_freqs = defaultdict(int)\n",
    "        self.model = {}\n",
    "        self._read_corpus()\n",
    "        self._calculate_frequencies()\n",
    "        self._build_model()\n",
    "\n",
    "    def _read_corpus(self):\n",
    "        with open(self.corpus_file_path, 'r', encoding='utf-8') as file:\n",
    "            self.corpus = file.readlines()\n",
    "\n",
    "    def _calculate_frequencies(self):\n",
    "        for text in self.corpus:\n",
    "            words_with_offsets = self.tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "            new_words = [word for word, offset in words_with_offsets]\n",
    "            for word in new_words:\n",
    "                self.word_freqs[word] += 1\n",
    "                for i in range(len(word)):\n",
    "                    self.char_freqs[word[i]] += self.word_freqs[word]\n",
    "                    for j in range(i + 2, len(word) + 1):\n",
    "                        self.subwords_freqs[word[i:j]] += self.word_freqs[word]\n",
    "\n",
    "    def _build_model(self):\n",
    "        total_sum = sum([freq for token, freq in self.char_freqs.items()])\n",
    "        token_freqs = list(self.char_freqs.items()) + sorted(self.subwords_freqs.items(), key=lambda x: x[1], reverse=True)\n",
    "        token_freqs = {token: freq for token, freq in token_freqs}\n",
    "        self.model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}\n",
    "\n",
    "    def encode_word(self, word):\n",
    "        best_segmentations = [{\"start\": 0, \"score\": 1}] + [{\"start\": None, \"score\": None} for _ in range(len(word))]\n",
    "        for start_idx in range(len(word)):\n",
    "            best_score_at_start = best_segmentations[start_idx][\"score\"]\n",
    "            for end_idx in range(start_idx + 1, len(word) + 1):\n",
    "                token = word[start_idx:end_idx]\n",
    "                if token in self.model and best_score_at_start is not None:\n",
    "                    score = self.model[token] + best_score_at_start\n",
    "                    if (best_segmentations[end_idx][\"score\"] is None or best_segmentations[end_idx][\"score\"] > score):\n",
    "                        best_segmentations[end_idx] = {\"start\": start_idx, \"score\": score}\n",
    "\n",
    "        segmentation = best_segmentations[-1]\n",
    "        if segmentation[\"score\"] is None:\n",
    "            return [\"<unk>\"], None\n",
    "\n",
    "        score = segmentation[\"score\"]\n",
    "        start = segmentation[\"start\"]\n",
    "        end = len(word)\n",
    "        tokens = []\n",
    "        while start != 0:\n",
    "            tokens.insert(0, word[start:end])\n",
    "            next_start = best_segmentations[start][\"start\"]\n",
    "            end = start\n",
    "            start = next_start\n",
    "        tokens.insert(0, word[start:end])\n",
    "        return tokens, score\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        words_with_offsets = self.tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "        pre_tokenized_text = [word for word, offset in words_with_offsets]\n",
    "        encoded_words = [self.encode_word(word)[0] for word in pre_tokenized_text]\n",
    "        return sum(encoded_words, [])\n",
    "\n",
    "    def compute_loss(self):\n",
    "        loss = 0\n",
    "        for word, freq in self.word_freqs.items():\n",
    "            _, word_loss = self.encode_word(word)\n",
    "            loss += freq * word_loss\n",
    "        return loss\n",
    "\n",
    "    def compute_scores(self):\n",
    "        scores = {}\n",
    "        model_loss = self.compute_loss()\n",
    "        for token, score in self.model.items():\n",
    "            if len(token) == 1:\n",
    "                continue\n",
    "            model_without_token = copy.deepcopy(self.model)\n",
    "            _ = model_without_token.pop(token)\n",
    "            scores[token] = self.compute_loss(model_without_token) - model_loss\n",
    "        return scores\n",
    "\n",
    "    def tokenize_file(self, input_file_path, output_file_path):\n",
    "        with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "\n",
    "        tokens = self.tokenize(text)\n",
    "\n",
    "        df = pd.DataFrame(tokens, columns=['Word'])\n",
    "        word_counts = df['Word'].value_counts().reset_index()\n",
    "        word_counts.columns = ['Word', 'Frequency']\n",
    "        df['Length'] = df['Word'].apply(len)\n",
    "        unique_words = df.drop_duplicates(subset=['Word'])\n",
    "        result = pd.merge(unique_words, word_counts, on='Word')\n",
    "        result.to_csv(output_file_path, index=False)\n",
    "\n",
    "\n",
    "# Usage\n",
    "model_name = \"xlm-roberta-base\"\n",
    "corpus_file_path = \"odia_sentences.txt\"\n",
    "tokenizer = UnigramTokenizer(model_name, corpus_file_path)\n",
    "tokenizer.tokenize_file(\"odia_sentences_text.txt\", \"Uni_val.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d7692ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "import pandas as pd\n",
    "from math import log\n",
    "\n",
    "class UnigramTokenizer:\n",
    "    def __init__(self, model_name, corpus_file_path):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.corpus_file_path = corpus_file_path\n",
    "        self.word_freqs = defaultdict(int)\n",
    "        self.char_freqs = defaultdict(int)\n",
    "        self.subwords_freqs = defaultdict(int)\n",
    "        self.model = {}\n",
    "        self._read_corpus()\n",
    "        self._calculate_frequencies()\n",
    "        self._build_model()\n",
    "\n",
    "    def _read_corpus(self):\n",
    "        with open(self.corpus_file_path, 'r', encoding='utf-8') as file:\n",
    "            self.corpus = file.readlines()\n",
    "\n",
    "    def _calculate_frequencies(self):\n",
    "        for text in self.corpus:\n",
    "            words_with_offsets = self.tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "            new_words = [word for word, offset in words_with_offsets]\n",
    "            for word in new_words:\n",
    "                self.word_freqs[word] += 1\n",
    "                for i in range(len(word)):\n",
    "                    self.char_freqs[word[i]] += self.word_freqs[word]\n",
    "                    for j in range(i + 2, len(word) + 1):\n",
    "                        self.subwords_freqs[word[i:j]] += self.word_freqs[word]\n",
    "\n",
    "    def _build_model(self):\n",
    "        total_sum = sum([freq for token, freq in self.char_freqs.items()])\n",
    "        token_freqs = list(self.char_freqs.items()) + sorted(self.subwords_freqs.items(), key=lambda x: x[1], reverse=True)\n",
    "        token_freqs = {token: freq for token, freq in token_freqs}\n",
    "        self.model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}\n",
    "\n",
    "    def encode_word(self, word):\n",
    "        best_segmentations = [{\"start\": 0, \"score\": 1}] + [{\"start\": None, \"score\": None} for _ in range(len(word))]\n",
    "        for start_idx in range(len(word)):\n",
    "            best_score_at_start = best_segmentations[start_idx][\"score\"]\n",
    "            for end_idx in range(start_idx + 1, len(word) + 1):\n",
    "                token = word[start_idx:end_idx]\n",
    "                if token in self.model and best_score_at_start is not None:\n",
    "                    score = self.model[token] + best_score_at_start\n",
    "                    if (best_segmentations[end_idx][\"score\"] is None or best_segmentations[end_idx][\"score\"] > score):\n",
    "                        best_segmentations[end_idx] = {\"start\": start_idx, \"score\": score}\n",
    "\n",
    "        segmentation = best_segmentations[-1]\n",
    "        if segmentation[\"score\"] is None:\n",
    "            return [\"<unk>\"], None\n",
    "\n",
    "        score = segmentation[\"score\"]\n",
    "        start = segmentation[\"start\"]\n",
    "        end = len(word)\n",
    "        tokens = []\n",
    "        while start != 0:\n",
    "            tokens.insert(0, word[start:end])\n",
    "            next_start = best_segmentations[start][\"start\"]\n",
    "            end = start\n",
    "            start = next_start\n",
    "        tokens.insert(0, word[start:end])\n",
    "        return tokens, score\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        words_with_offsets = self.tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "        pre_tokenized_text = [word for word, offset in words_with_offsets]\n",
    "        encoded_words = [self.encode_word(word)[0] for word in pre_tokenized_text]\n",
    "        return sum(encoded_words, [])\n",
    "\n",
    "    def compute_loss(self):\n",
    "        loss = 0\n",
    "        for word, freq in self.word_freqs.items():\n",
    "            _, word_loss = self.encode_word(word)\n",
    "            loss += freq * word_loss\n",
    "        return loss\n",
    "\n",
    "    def compute_scores(self):\n",
    "        scores = {}\n",
    "        model_loss = self.compute_loss()\n",
    "        for token, score in self.model.items():\n",
    "            if len(token) == 1:\n",
    "                continue\n",
    "            model_without_token = copy.deepcopy(self.model)\n",
    "            _ = model_without_token.pop(token)\n",
    "            scores[token] = self.compute_loss(model_without_token) - model_loss\n",
    "        return scores\n",
    "\n",
    "    def tokenize_file(self, input_file_path, output_file_path):\n",
    "        with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "\n",
    "        tokens = self.tokenize(text)\n",
    "\n",
    "        df = pd.DataFrame(tokens, columns=['Word'])\n",
    "        word_counts = df['Word'].value_counts().reset_index()\n",
    "        word_counts.columns = ['Word', 'Frequency']\n",
    "        df['Length'] = df['Word'].apply(len)\n",
    "        unique_words = df.drop_duplicates(subset=['Word'])\n",
    "        result = pd.merge(unique_words, word_counts, on='Word')\n",
    "        result.to_csv(output_file_path, index=False)\n",
    "\n",
    "\n",
    "# Usage\n",
    "model_name = \"xlm-roberta-base\"\n",
    "corpus_file_path = \"odia_sentences.txt\"\n",
    "tokenizer = UnigramTokenizer(model_name, corpus_file_path)\n",
    "tokenizer.tokenize_file(\"odia_sentences_text.txt\", \"Uni_val.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd302dd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
